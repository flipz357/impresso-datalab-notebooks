{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4982c90e1ce34f5f",
   "metadata": {},
   "source": [
    "# Named Entity Recognition and Linking with Impresso BERT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a706a1074ca4c",
   "metadata": {},
   "source": [
    "## Good to know before starting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8f8cbb23dbebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2c93c-a027-46f1-9dc7-0cd97e6c474b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa54e9-a581-4ebb-8094-388b40da73d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddfc8e-7b7a-40a3-9379-65a9372eb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Define the model name to be used for token classification, we use the Impresso NER\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual\"\n",
    "\n",
    "# Load the tokenizer corresponding to the specified model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04b662-a9b2-452b-8bb4-aa99213b9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for named entity recognition (NER) using the loaded model and tokenizer\n",
    "nlp = pipeline(\"generic-ner\", model=MODEL_NAME, tokenizer=tokenizer, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caddbcd-ef1e-42ae-ac00-aee7da74ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nicely(results):\n",
    "    for key, entities in results.items():\n",
    "        if entities:\n",
    "            print(f\"\\n**{key}**\\n\")\n",
    "            print(f\"{'Entity':<15} {'Type':<10} {'Score':<8} {'Index':<5} {'Word':<20} {'Start':<5} {'End':<5}\")\n",
    "            print(\"-\" * 70)\n",
    "            for entity in entities:\n",
    "                print(f\"{entity['word']:<15} {entity['entity']:<10} {entity['score']:<8.4f} {entity['index']:<5} {entity['word']:<20} {entity['start']:<5} {entity['end']:<5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0cf70-fd9a-4e82-a5fa-3d5bcecef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"\"\"Apple est créée le 1er avril 1976 dans le garage de la maison \n",
    "            d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak \n",
    "            et Ronald Wayne, puis constituée sous forme de société le 3 janvier 1977 à l'origine \n",
    "            sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification \n",
    "            de ses produits, le mot « computer » est retiré le 9 janvier 2015.\n",
    "            \"\"\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Input text to be processed for named entity recognition\n",
    "    print(f'Sentence: {sentence}')\n",
    "    results = nlp(sentence)\n",
    "    print_nicely(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e61e17-ff51-490a-9756-ea00241637a0",
   "metadata": {},
   "source": [
    "Entity Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170c79d-7a3b-49ed-8ff9-8f1d5dc92a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entity_tags(text, results):\n",
    "    entities = []\n",
    "    for task, ents in results.items():\n",
    "        for entity in ents:\n",
    "            entities.append((entity['start'], entity['end'], entity['word']))\n",
    "\n",
    "    already_done = []\n",
    "    # Insert tags in the text using replace method\n",
    "    for start, end, word in entities:\n",
    "        if word not in already_done:\n",
    "            text = text.replace(word, f\"[START] {word} [END]\")\n",
    "            already_done.append(word)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b132e2-acc8-4da3-be6a-bae0233a0729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a2d1a-56bb-4d1d-ac19-5987b8ccb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules from the transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model from the specified pre-trained model name\n",
    "# The model used here is \"impresso-project/nel-mgenre-multilingual\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"impresso-project/nel-mgenre-multilingual\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"impresso-project/nel-mgenre-multilingual\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29dd95-191e-4841-bdb6-06166aaaddf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1496a-7c4b-4e09-83d6-4d2aa5c5a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each sentence for named entity recognition and linking\n",
    "for sentence in sentences:\n",
    "    # Input text to be processed for named entity recognition\n",
    "    print(f'Sentence: {sentence}')\n",
    "    \n",
    "    # Run the NER pipeline on the input sentence and store the results\n",
    "    results = nlp(sentence)\n",
    "\n",
    "    # Initialize a list to hold the entities\n",
    "    entities = []\n",
    "\n",
    "    # Extract entities from the results\n",
    "    for task, ents in results.items():\n",
    "        for entity in ents:\n",
    "            entities.append((entity['start'], entity['end'], entity['word']))\n",
    "    \n",
    "    # List to keep track of already processed words to avoid duplicate tagging\n",
    "    already_done = []\n",
    "\n",
    "    # Process each entity for linking\n",
    "    for start, end, word in entities:\n",
    "        if word not in already_done:\n",
    "            # Tag the entity in the text\n",
    "            entity_text = sentence.replace(word, f\"[START] {word} [END]\")\n",
    "            print(f\"\\nEntity: {word}, Tagged Text: {entity_text}\\n\")\n",
    "\n",
    "            # Generate Wikipedia links for the tagged text\n",
    "            outputs = model.generate(\n",
    "                **tokenizer([entity_text], return_tensors=\"pt\"),\n",
    "                num_beams=5,\n",
    "                num_return_sequences=5\n",
    "            )\n",
    "            \n",
    "            # Decode the generated output to get the Wikipedia links\n",
    "            wikipedia_links = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            print(f\"\\nEntity: {word}, Wikipedia Links: {wikipedia_links}\")\n",
    "            \n",
    "            # Add the word to the already processed list\n",
    "            already_done.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5b742-26af-4017-b204-12a0e460bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_wikipedia_page_props\n",
    "\n",
    "# Process each sentence for named entity recognition and linking\n",
    "for sentence in sentences:\n",
    "    # Input text to be processed for named entity recognition\n",
    "    print(f'Sentence: {sentence}')\n",
    "    \n",
    "    # Run the NER pipeline on the input sentence and store the results\n",
    "    results = nlp(sentence)\n",
    "\n",
    "    # Initialize a list to hold the entities\n",
    "    entities = []\n",
    "\n",
    "    # Extract entities from the results\n",
    "    for task, ents in results.items():\n",
    "        for entity in ents:\n",
    "            entities.append((entity['start'], entity['end'], entity['word']))\n",
    "    \n",
    "    # List to keep track of already processed words to avoid duplicate tagging\n",
    "    already_done = []\n",
    "\n",
    "    # Process each entity for linking\n",
    "    for start, end, word in entities:\n",
    "        if word not in already_done:\n",
    "            # Tag the entity in the text\n",
    "            entity_text = sentence.replace(word, f\"[START] {word} [END]\")\n",
    "            print(f\"\\nEntity: {word}, Tagged Text: {entity_text}\\n\")\n",
    "\n",
    "            # Generate Wikipedia links for the tagged text\n",
    "            outputs = model.generate(\n",
    "                **tokenizer([entity_text], return_tensors=\"pt\"),\n",
    "                num_beams=5,\n",
    "                num_return_sequences=5\n",
    "            )\n",
    "            \n",
    "            # Decode the generated output to get the Wikipedia links\n",
    "            wikipedia_links = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            print(f\"\\nEntity: {word}, Wikipedia Links: {wikipedia_links}\")\n",
    "            \n",
    "            # Add the word to the already processed list\n",
    "            already_done.append(word)\n",
    "            \n",
    "            # Retrieve and print Wikidata QID for each Wikipedia link\n",
    "            for wikipedia_link in wikipedia_links:\n",
    "                qid = get_wikipedia_page_props(wikipedia_link)\n",
    "                print(f\"  Wikidata: {wikipedia_link} -> {qid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0238bf-c4d6-4bc3-b4ed-df8b05c0b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdb43f-da8e-4f49-8a61-325bd2289ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725503fb-acdc-4c0e-abf4-ecdd56718acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
