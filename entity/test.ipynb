{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d2a5f4d-e8c4-4902-8ab1-0aa2cb01e110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- configuration_stacked.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c974c7924b4ae785071103098c1252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generic_ner.py:   0%|          | 0.00/23.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- generic_ner.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- modeling_stacked.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'time.date.abs', 'confidence_ner': 85.0, 'index': (0, 5), 'surface': \"En l'an 1348\", 'lOffset': 0, 'rOffset': 12}, {'type': 'loc.adm.nat', 'confidence_ner': 90.75, 'index': (19, 20), 'surface': 'Europe', 'lOffset': 69, 'rOffset': 75}, {'type': 'loc', 'confidence_ner': 75.45, 'index': (22, 25), 'surface': 'Royaume de France', 'lOffset': 80, 'rOffset': 97}, {'type': 'pers.ind', 'confidence_ner': 85.27, 'index': (44, 47), 'surface': 'roi Philippe VI', 'lOffset': 181, 'rOffset': 196}, {'type': 'loc.adm.town', 'confidence_ner': 30.59, 'index': (51, 52), 'surface': 'Louvre', 'lOffset': 210, 'rOffset': 216}, {'type': 'loc.adm.town', 'confidence_ner': 94.46, 'index': (60, 61), 'surface': 'Paris', 'lOffset': 266, 'rOffset': 271}, {'type': 'pers.ind', 'confidence_ner': 96.1, 'index': (77, 81), 'surface': 'chancelier Guillaume de Nogaret', 'lOffset': 350, 'rOffset': 381}, {'type': 'loc.adm.nat', 'confidence_ner': 49.35, 'index': (22, 23), 'surface': 'Royaume', 'lOffset': 80, 'rOffset': 87}, {'type': 'loc.adm.nat', 'confidence_ner': 24.18, 'index': (24, 25), 'surface': 'France', 'lOffset': 91, 'rOffset': 97}]\n",
      "En l'an 1348 --- En l'an 1348 \n",
      "\n",
      "Europe --- Europe \n",
      "\n",
      "Royaume de France --- Royaume de France \n",
      "\n",
      "roi Philippe VI --- roi Philippe VI \n",
      "\n",
      "Louvre --- Louvre \n",
      "\n",
      "Paris --- Paris \n",
      "\n",
      "chancelier Guillaume de Nogaret --- chancelier Guillaume de Nogaret \n",
      "\n",
      "Royaume --- Royaume \n",
      "\n",
      "France --- France \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Python modules from the Transformers library\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the model name to be used for token classification, we use the Impresso NER\n",
    "# that can be found at \"https://huggingface.co/impresso-project/ner-stacked-bert-multilingual\"\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual\"\n",
    "\n",
    "# Load the tokenizer corresponding to the specified model name\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(\"generic-ner\", model=MODEL_NAME, \n",
    "                        tokenizer=ner_tokenizer, \n",
    "                        trust_remote_code=True,\n",
    "                        device='cpu')\n",
    "\n",
    "sentence = \"En l'an 1348, au plus fort des ravages de la peste noire à travers l'Europe, le Royaume de France se trouvait à la fois au bord du désespoir et face à une opportunité. À la cour du roi Philippe VI, les murs du Louvre étaient animés par les rapports sombres venus de Paris et des villes environnantes. La peste ne montrait aucun signe de répit, et le chancelier Guillaume de Nogaret, le conseiller le plus fidèle du roi, portait le lourd fardeau de gérer la survie du royaume.\"\n",
    "\n",
    "entities = ner_pipeline(sentence)\n",
    "print(entities)\n",
    "for entity in entities:\n",
    "    print(sentence[entity['lOffset']: entity['rOffset']], '---', entity['surface'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dd6f68e-d25e-4efa-87e9-62a2d72ca3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial entities: {'NE-COARSE-LIT': [{'type': 'pers', 'confidence_ner': 86.98, 'index': (0, 5), 'surface': 'Emily, the known actress', 'lOffset': 0, 'rOffset': 24}, {'type': 'loc', 'confidence_ner': 96.33, 'index': (9, 10), 'surface': 'Paris', 'lOffset': 38, 'rOffset': 43}, {'type': 'loc', 'confidence_ner': 97.24, 'index': (12, 13), 'surface': 'Seine', 'lOffset': 53, 'rOffset': 58}], 'NE-COARSE-METO': [], 'NE-FINE-COMP': [{'type': 'comp.name', 'confidence_ner': 55.02, 'index': (0, 1), 'surface': 'Emily', 'lOffset': 0, 'rOffset': 5}, {'type': 'comp.function', 'confidence_ner': 71.01, 'index': (3, 4), 'surface': 'known', 'lOffset': 11, 'rOffset': 16}, {'type': 'comp.function', 'confidence_ner': 61.28, 'index': (4, 5), 'surface': 'actress', 'lOffset': 17, 'rOffset': 24}], 'NE-FINE-LIT': [{'type': 'pers.ind', 'confidence_ner': 76.7, 'index': (0, 5), 'surface': 'Emily, the known actress', 'lOffset': 0, 'rOffset': 24}, {'type': 'loc.adm.town', 'confidence_ner': 92.75, 'index': (9, 10), 'surface': 'Paris', 'lOffset': 38, 'rOffset': 43}, {'type': 'loc.adm.reg', 'confidence_ner': 38.55, 'index': (12, 13), 'surface': 'Seine', 'lOffset': 53, 'rOffset': 58}], 'NE-FINE-METO': [], 'NE-NESTED': []}\n",
      "Initial entities: [{'type': 'pers', 'confidence_ner': 86.98, 'index': (0, 5), 'surface': 'Emily, the known actress', 'lOffset': 0, 'rOffset': 24}, {'type': 'loc', 'confidence_ner': 96.33, 'index': (9, 10), 'surface': 'Paris', 'lOffset': 38, 'rOffset': 43}, {'type': 'loc', 'confidence_ner': 97.24, 'index': (12, 13), 'surface': 'Seine', 'lOffset': 53, 'rOffset': 58}, {'type': 'comp.name', 'confidence_ner': 55.02, 'index': (0, 1), 'surface': 'Emily', 'lOffset': 0, 'rOffset': 5}, {'type': 'comp.function', 'confidence_ner': 71.01, 'index': (3, 4), 'surface': 'known', 'lOffset': 11, 'rOffset': 16}, {'type': 'comp.function', 'confidence_ner': 61.28, 'index': (4, 5), 'surface': 'actress', 'lOffset': 17, 'rOffset': 24}, {'type': 'pers.ind', 'confidence_ner': 76.7, 'index': (0, 5), 'surface': 'Emily, the known actress', 'lOffset': 0, 'rOffset': 24}, {'type': 'loc.adm.town', 'confidence_ner': 92.75, 'index': (9, 10), 'surface': 'Paris', 'lOffset': 38, 'rOffset': 43}, {'type': 'loc.adm.reg', 'confidence_ner': 38.55, 'index': (12, 13), 'surface': 'Seine', 'lOffset': 53, 'rOffset': 58}]\n",
      "[{'type': 'pers.ind', 'confidence_ner': 76.7, 'index': (0, 5), 'surface': 'Emily, the known actress', 'lOffset': 0, 'rOffset': 24}, {'type': 'loc.adm.town', 'confidence_ner': 92.75, 'index': (9, 10), 'surface': 'Paris', 'lOffset': 38, 'rOffset': 43}, {'type': 'loc.adm.reg', 'confidence_ner': 38.55, 'index': (12, 13), 'surface': 'Seine', 'lOffset': 53, 'rOffset': 58}]\n",
      "Emily, the known actress --- Emily, the known actress \n",
      "\n",
      "Paris --- Paris \n",
      "\n",
      "Seine --- Seine \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Python modules from the Transformers library\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the model name to be used for token classification, we use the Impresso NER\n",
    "# that can be found at \"https://huggingface.co/impresso-project/ner-stacked-bert-multilingual\"\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual\"\n",
    "\n",
    "# Load the tokenizer corresponding to the specified model name\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(\"generic-ner\", model=MODEL_NAME, \n",
    "                        tokenizer=ner_tokenizer, \n",
    "                        trust_remote_code=True,\n",
    "                        device='cpu')\n",
    "\n",
    "sentence = \"En l'an 1348, au plus fort des ravages de la peste noire à travers l'Europe, le Royaume de France se trouvait à la fois au bord du désespoir et face à une opportunité. À la cour du roi Philippe VI, les murs du Louvre étaient animés par les rapports sombres venus de Paris et des villes environnantes. La peste ne montrait aucun signe de répit, et le chancelier Guillaume de Nogaret, le conseiller le plus fidèle du roi, portait le lourd fardeau de gérer la survie du royaume.\"\n",
    "sentence = \"Emily, the known actress, is going to Paris near the Seine.\"\n",
    "entities = ner_pipeline(sentence)\n",
    "print(entities)\n",
    "for entity in entities:\n",
    "    print(sentence[entity['lOffset']: entity['rOffset']], '---', entity['surface'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a40b9415-c0b7-45b5-842f-2c8a394a8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- configuration_stacked.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfd6964973d4382aec0f149db5216df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generic_ner.py:   0%|          | 0.00/23.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- generic_ner.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- modeling_stacked.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'loc', 'confidence_ner': 75.45, 'index': (22, 25), 'surface': 'Royaume de France', 'lOffset': 80, 'rOffset': 97}]\n",
      "Royaume de France --- Royaume de France \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Python modules from the Transformers library\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the model name to be used for token classification, we use the Impresso NER\n",
    "# that can be found at \"https://huggingface.co/impresso-project/ner-stacked-bert-multilingual\"\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual\"\n",
    "\n",
    "# Load the tokenizer corresponding to the specified model name\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(\"generic-ner\", model=MODEL_NAME, \n",
    "                        tokenizer=ner_tokenizer, \n",
    "                        trust_remote_code=True,\n",
    "                        device='cpu')\n",
    "\n",
    "sentence = \"En l'an 1348, au plus fort des ravages de la peste noire à travers l'Europe, le Royaume de France se trouvait à la fois au bord du désespoir et face à une opportunité. À la cour du roi Philippe VI, les murs du Louvre étaient animés par les rapports sombres venus de Paris et des villes environnantes. La peste ne montrait aucun signe de répit, et le chancelier Guillaume de Nogaret, le conseiller le plus fidèle du roi, portait le lourd fardeau de gérer la survie du royaume.\"\n",
    "\n",
    "entities = ner_pipeline(sentence)\n",
    "print(entities)\n",
    "for entity in entities:\n",
    "    print(sentence[entity['lOffset']: entity['rOffset']], '---', entity['surface'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2781a-8074-43fc-9a50-da0a7c7ccf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print each entry nicely\n",
    "def print_nicely(data):\n",
    "    for idx, entry in enumerate(data, start=1):\n",
    "        for key, value in entry.items():\n",
    "            print(f\"  {key.capitalize()}: {value}\")\n",
    "        print()  # Blank line between entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193c3ba-7d31-477b-ae96-38b3e0db18f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d99298-51ee-4069-a05a-04b50ec61a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Type: loc.adm.nat\n",
      "  Score: 16.29\n",
      "  Index: (4, 5)\n",
      "  Surface: Atlantic\n",
      "  Start: 22\n",
      "  End: 30\n",
      "  Loffset: 0\n",
      "  Roffset: 8\n",
      "\n",
      "  Type: pers.ind\n",
      "  Score: 92.82\n",
      "  Index: (6, 8)\n",
      "  Surface: George Washington\n",
      "  Start: 32\n",
      "  End: 49\n",
      "  Loffset: 0\n",
      "  Roffset: 17\n",
      "  Function: secretary\n",
      "  Name: George Washington\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f513ca8b-4e8a-46c1-948c-cfc26344d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Type: pers.ind\n",
      "  Score: 91.86\n",
      "  Index: (0, 10)\n",
      "  Surface: Thomas Jefferson, the nation's Secretary of State\n",
      "  Start: 0\n",
      "  End: 49\n",
      "  Loffset: 0\n",
      "  Roffset: 49\n",
      "  Name: Thomas Jefferson\n",
      "  Function: nation's Secretary of State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Thomas Jefferson, the nation's Secretary of State, played a key role in drafting policies for the new American government.\"\n",
    "# Function to print each entry nicely\n",
    "def print_nicely(data):\n",
    "    for idx, entry in enumerate(data, start=1):\n",
    "        for key, value in entry.items():\n",
    "            print(f\"  {key.capitalize()}: {value}\")\n",
    "        print()  # Blank line between entries\n",
    "\n",
    "entities = ner_pipeline(sentence)\n",
    "print_nicely(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb44dcd2-1089-4498-8482-85ee2ef4b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Type: pers.ind\n",
      "  Score: 91.86\n",
      "  Index: (0, 10)\n",
      "  Surface: Thomas Jefferson, the nation's Secretary of State\n",
      "  Start: 0\n",
      "  End: 49\n",
      "  Loffset: 0\n",
      "  Roffset: 49\n",
      "  Name: Thomas Jefferson\n",
      "  Function: nation's Secretary of State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Thomas Jefferson, the nation's Secretary of State, played a key role in drafting policies for the new American government.\"\n",
    "# Function to print each entry nicely\n",
    "def print_nicely(data):\n",
    "    for idx, entry in enumerate(data, start=1):\n",
    "        for key, value in entry.items():\n",
    "            print(f\"  {key.capitalize()}: {value}\")\n",
    "        print()  # Blank line between entries\n",
    "\n",
    "entities = ner_pipeline(sentence)\n",
    "print_nicely(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff202221-03a8-4d5b-8384-6c5683d182bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Type: loc.adm.nat\n",
      "  Score: 16.14\n",
      "  Index: (4, 5)\n",
      "  Surface: Atlantic\n",
      "  Start: 22\n",
      "  End: 30\n",
      "  Loffset: 0\n",
      "  Roffset: 8\n",
      "\n",
      "  Type: pers.ind\n",
      "  Score: 80.22\n",
      "  Index: (6, 8)\n",
      "  Surface: George Washington\n",
      "  Start: 32\n",
      "  End: 49\n",
      "  Loffset: 0\n",
      "  Roffset: 17\n",
      "  Name: George Washington\n",
      "  Function: President of the United States\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Meanwhile, across the Atlantic, George Washington, the first President of the United States, was shaping policies.\"\n",
    "# Function to print each entry nicely\n",
    "def print_nicely(data):\n",
    "    for idx, entry in enumerate(data, start=1):\n",
    "        for key, value in entry.items():\n",
    "            print(f\"  {key.capitalize()}: {value}\")\n",
    "        print()  # Blank line between entries\n",
    "\n",
    "entities = ner_pipeline(sentence)\n",
    "print_nicely(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb3998f-aa6f-4011-bfb1-bc9911428f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'loc.adm.nat',\n",
       "  'score': 16.14,\n",
       "  'index': (4, 5),\n",
       "  'surface': 'Atlantic',\n",
       "  'start': 22,\n",
       "  'end': 30,\n",
       "  'lOffset': 0,\n",
       "  'rOffset': 8},\n",
       " {'type': 'pers.ind',\n",
       "  'score': 80.22,\n",
       "  'index': (6, 8),\n",
       "  'surface': 'George Washington',\n",
       "  'start': 32,\n",
       "  'end': 49,\n",
       "  'lOffset': 0,\n",
       "  'rOffset': 17,\n",
       "  'name': 'George Washington',\n",
       "  'function': 'President of the United States'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c76e826-2cec-46ce-b069-929ec8c87cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda5089e4496438ebe1b33ea80bc9840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  82%|########2 | 2.03G/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7c6ced14ac4eb595de4706bd971ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['sequences', 'scores', 'past_key_values']) !!!!\n",
      "Linked Entity Output:\n",
      "[{'surface': 'Palace of Versailles', 'wkpedia_pagename': 'Palace of Versailles', 'wkd_id': 'Q2946', 'url': 'https://en.wikipedia.org/wiki/Palace_of_Versailles', 'type': 'UNK', 'confidence_nel': 99.99, 'lOffset': 33, 'rOffset': 55}]\n",
      "\n",
      "Time taken for NEL processing: 6.8193 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "NEL_MODEL_NAME = \"impresso-project/nel-mgenre-multilingual\"\n",
    "\n",
    "# Load the tokenizer and model from the specified pre-trained model name\n",
    "nel_tokenizer = AutoTokenizer.from_pretrained(\"impresso-project/nel-mgenre-multilingual\")\n",
    "\n",
    "nel_pipeline = pipeline(\"generic-nel\", model=NEL_MODEL_NAME, tokenizer=nel_tokenizer, trust_remote_code=True, device='cpu')\n",
    "\n",
    "simple_sentence = \"The event was held at the [START] Palace of Versailles [END], a symbol of French monarchy.\"\n",
    "\n",
    "# Start time computation\n",
    "start_time = time.time()\n",
    "\n",
    "simple_sentence = \"The event was held at the [START] Palace of Versailles [END], a symbol of French monarchy.\"\n",
    "linked_entity = nel_pipeline(simple_sentence)\n",
    "\n",
    "# End time computation\n",
    "end_time = time.time()\n",
    "\n",
    "# Time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "def print_nicely(linked_entity, time_taken):\n",
    "    print(\"Linked Entity Output:\")\n",
    "    print(linked_entity)\n",
    "    print(f\"\\nTime taken for NEL processing: {time_taken:.4f} seconds\")\n",
    "\n",
    "# Call the function to display the result and time\n",
    "print_nicely(linked_entity, time_taken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0760883-fb15-48e5-9b7b-c509ca3920df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdc8e1cd6c146578a09be4ab10ba3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generic_nel.py:   0%|          | 0.00/6.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/nel-mgenre-multilingual:\n",
      "- generic_nel.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Un1ted Press [END] - On the h0me fr0nt, the British p0pulace remains steadfast in the f4ce of 0ngoing air raids.\n",
      "[{'surface': 'Un1ted Press', 'wkd_id': 'Q493845', 'wkpedia_pagename': 'United Press International', 'wkpedia_url': 'https://en.wikipedia.org/wiki/United_Press_International', 'type': 'UNK', 'confidence_nel': 55.89, 'lOffset': 7, 'rOffset': 21}]\n",
      "In [START] Lon6on [END], trotz d3r Zerstörung, ist der Geist der M3nschen ungeb4ochen, mit Freiwilligen und zivilen Verteidigungseinheiten, die unermüdlich arbeiten, um die Kriegsanstrengungen zu unterstützen.\n",
      "[{'surface': 'Lon6on', 'wkd_id': 'Q84', 'wkpedia_pagename': 'London', 'wkpedia_url': 'https://de.wikipedia.org/wiki/London', 'type': 'UNK', 'confidence_nel': 99.99, 'lOffset': 10, 'rOffset': 18}]\n",
      "Les rapports des correspondants de la [START] AFP [END] mettent en lumiére la poussée nationale pour augmenter la production dans les usines, essentielle pour fournir au front les matériaux nécessaires à la victoire.\n",
      "[{'surface': 'AFP', 'wkd_id': 'Q40464', 'wkpedia_pagename': 'Agence France-Presse', 'wkpedia_url': 'https://fr.wikipedia.org/wiki/Agence_France-Presse', 'type': 'UNK', 'confidence_nel': 100.0, 'lOffset': 45, 'rOffset': 50}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "\n",
    "NEL_MODEL_NAME = \"impresso-project/nel-mgenre-multilingual\"\n",
    "\n",
    "# Load the tokenizer and model from the specified pre-trained model name\n",
    "# The model used here is \"https://huggingface.co/impresso-project/nel-mgenre-multilingual\"\n",
    "nel_tokenizer = AutoTokenizer.from_pretrained(\"impresso-project/nel-mgenre-multilingual\")\n",
    "\n",
    "sentences = [\"[START] Un1ted Press [END] - On the h0me fr0nt, the British p0pulace remains steadfast in the f4ce of 0ngoing air raids.\",\n",
    "             \"In [START] Lon6on [END], trotz d3r Zerstörung, ist der Geist der M3nschen ungeb4ochen, mit Freiwilligen und zivilen Verteidigungseinheiten, die unermüdlich arbeiten, um die Kriegsanstrengungen zu unterstützen.\",\n",
    "             \"Les rapports des correspondants de la [START] AFP [END] mettent en lumiére la poussée nationale pour augmenter la production dans les usines, essentielle pour fournir au front les matériaux nécessaires à la victoire.\"]\n",
    "\n",
    "nel_pipeline = pipeline(\"generic-nel\", model=NEL_MODEL_NAME, \n",
    "                        tokenizer=nel_tokenizer, \n",
    "                        trust_remote_code=True,\n",
    "                        device='cpu')\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    linked_entity = nel_pipeline(sentence)\n",
    "    print(linked_entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69d68de-5496-4443-9167-1eb79f11fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- configuration_stacked.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3d2a4fb21149008e615bd2c8f948ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generic_ner.py:   0%|          | 0.00/18.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- generic_ner.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/ner-stacked-bert-multilingual:\n",
      "- modeling_stacked.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the year 1789, King Louis XVI, ruler of France, convened the Estates-General at the Palace of Versailles, \n",
      "                where Marie Antoinette, the Queen of France, alongside Maximilien Robespierre, a leading member of the National Assembly, \n",
      "                debated with Jean-Jacques Rousseau, the famous philosopher, and Charles de Talleyrand, the Bishop of Autun, \n",
      "                regarding the future of the French monarchy. At the same time, across the Atlantic in Philadelphia, \n",
      "                George Washington, the first President of the United States, and Thomas Jefferson, the nation's Secretary of State, \n",
      "                were drafting policies for the newly established American government following the signing of the Constitution.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m results \u001b[38;5;241m=\u001b[39m ner_pipeline(sentence)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Extract coarse and fine entities\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Visualize the coarse entities\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     print_nicely(results[key])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Define the model name to be used for token classification, we use the Impresso NER\n",
    "# that can be found at \"https://huggingface.co/impresso-project/ner-stacked-bert-multilingual\"\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual\"\n",
    "\n",
    "# Load the tokenizer corresponding to the specified model name\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(\"generic-ner\", model=MODEL_NAME, \n",
    "                        tokenizer=ner_tokenizer, \n",
    "                        trust_remote_code=True,\n",
    "                        device='cpu')\n",
    "sentences = [\"\"\"In the year 1789, King Louis XVI, ruler of France, convened the Estates-General at the Palace of Versailles, \n",
    "                where Marie Antoinette, the Queen of France, alongside Maximilien Robespierre, a leading member of the National Assembly, \n",
    "                debated with Jean-Jacques Rousseau, the famous philosopher, and Charles de Talleyrand, the Bishop of Autun, \n",
    "                regarding the future of the French monarchy. At the same time, across the Atlantic in Philadelphia, \n",
    "                George Washington, the first President of the United States, and Thomas Jefferson, the nation's Secretary of State, \n",
    "                were drafting policies for the newly established American government following the signing of the Constitution.\"\"\"]\n",
    "\n",
    "print(sentences[0])\n",
    "\n",
    "# Helper function to print entities one per row\n",
    "def print_nicely(entities):\n",
    "    for entity in entities:\n",
    "        print(f\"Entity: {entity['entity']} | Confidence: {entity['score']:.2f}% | Text: {entity['word'].strip()} | Start: {entity['start']} | End: {entity['end']}\")\n",
    "\n",
    "# Visualize stacked entities for each sentence\n",
    "for sentence in sentences:\n",
    "    results = ner_pipeline(sentence)\n",
    "\n",
    "    # Extract coarse and fine entities\n",
    "    for key in results.keys():\n",
    "        # Visualize the coarse entities\n",
    "        print_nicely(results[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e642c-fbc8-4bf4-8f35-8044107b8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sentence = \"The event was held at the [START] Palace of Versaittes [END], a symbol of French monarchy.\"\n",
    "\n",
    "# Start time computation\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the function to get the linked entity\n",
    "linked_entity = nel_pipeline(simple_sentence)\n",
    "\n",
    "# End time computation\n",
    "end_time = time.time()\n",
    "\n",
    "# Time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "def print_nicely(linked_entity, time_taken):\n",
    "    print(\"Linked Entity Output:\")\n",
    "    print(linked_entity)\n",
    "    print(f\"\\nTime taken for NEL processing: {time_taken:.4f} seconds\")\n",
    "\n",
    "# Call the function to display the result and time\n",
    "print_nicely(linked_entity, time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb8e349-8e51-47aa-800e-2ca527aa8d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb29db16875c4a319a737a6b1163752c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529d96d385974ca89552d8b96327652e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/impresso-project/nel-mgenre-multilingual:\n",
      "- generic_nel.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/eboros/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013b3df5398c4bdd842e452d73a20ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model from the specified pre-trained model name\u001b[39;00m\n\u001b[1;32m      7\u001b[0m nel_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpresso-project/nel-mgenre-multilingual\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m nel_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric-nel\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mNEL_MODEL_NAME, \n\u001b[1;32m     10\u001b[0m                         tokenizer\u001b[38;5;241m=\u001b[39mnel_tokenizer,\n\u001b[1;32m     11\u001b[0m                         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m simple_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[START]Robert Smith[END] was an influential figure in the 19th-century Pre-Raphaelite art movement.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Start time computation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    897\u001b[0m         model,\n\u001b[1;32m    898\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[1;32m    899\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    900\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[1;32m    901\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:288\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    290\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3604\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3589\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3590\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3592\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3603\u001b[0m     }\n\u001b[0;32m-> 3604\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[1;32m   3606\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3607\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3609\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1237\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1238\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1239\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1240\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1242\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1243\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1244\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1245\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1248\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1249\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1381\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1382\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1383\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[1;32m   1384\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[1;32m   1385\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1386\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1387\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1388\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1389\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1390\u001b[0m     )\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     http_get(\n\u001b[1;32m   1916\u001b[0m         url_to_download,\n\u001b[1;32m   1917\u001b[0m         f,\n\u001b[1;32m   1918\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1919\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1920\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1921\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1922\u001b[0m     )\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "NEL_MODEL_NAME = \"impresso-project/nel-mgenre-multilingual\"\n",
    "\n",
    "# Load the tokenizer and model from the specified pre-trained model name\n",
    "nel_tokenizer = AutoTokenizer.from_pretrained(\"impresso-project/nel-mgenre-multilingual\")\n",
    "\n",
    "nel_pipeline = pipeline(\"generic-nel\", model=NEL_MODEL_NAME, \n",
    "                        tokenizer=nel_tokenizer,\n",
    "                        trust_remote_code=True,\n",
    "                        device='cpu')\n",
    "\n",
    "simple_sentence = \"[START]Robert Smith[END] was an influential figure in the 19th-century Pre-Raphaelite art movement.\"\n",
    "\n",
    "# Start time computation\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the function to get the linked entity\n",
    "linked_entity = nel_pipeline(simple_sentence)\n",
    "\n",
    "# End time computation\n",
    "end_time = time.time()\n",
    "\n",
    "# Time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "def print_nicely(linked_entity, time_taken):\n",
    "    print(\"Linked Entity Output:\")\n",
    "    print(linked_entity)\n",
    "    print(f\"\\nTime taken for NEL processing: {time_taken:.4f} seconds\")\n",
    "\n",
    "# Call the function to display the result and time\n",
    "print_nicely(linked_entity, time_taken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4d0f3-4081-47b7-bcf9-8e952d650f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3304b-db1b-4496-996b-a145385401e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe8797-69b8-4f9f-b6b1-293ceb2aa421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17774b-09e8-4ceb-8ac4-4302e1848701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a67a7-d8c2-4a24-b37d-549dfdf154a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f22dc-b59c-4fbe-b2d2-182d7f989543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c3437-9640-4589-bc9a-7bf455437ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956f81d-da49-4af5-9653-fc3e505b5cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2c0d4-5798-43e4-8931-1f103c8616bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
