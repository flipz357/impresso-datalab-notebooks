{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P374GZ3Nxusx"
   },
   "source": [
    "# Detect Entities and Link them to Wikipedia and Wikidata in a Text through the Impresso API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSY3E8j5xusy"
   },
   "source": [
    "We refer to \"named entity recognition\" as NER, which is a tool that recognises entities such as persons and locations from text. A \"named entity linker\" (NEL) connects these entities to an existing one such as a real person that can be found on Wikipedia (with a unique id in Wikidata). Wikipedia is a free, user-edited encyclopedia with articles on a wide range of topics like historical events, famous people, or scientific concepts. Wikidata is a sister project of Wikipedia that stores structured data, like facts and relationships between entities, used for tasks where computers need to understand and process data, such as NER and NEL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if8onHfRxusz"
   },
   "source": [
    "In the context of _Impresso_, the NER tool was trained on the [HIPE 2020](https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-hipe2020.md) dataset. It was trained to recognise coarse and fine grained entities such as persons and locations, but also their names, titles, and functions. Further, the _Impresso_ NEL tool links these entity mentions to unique referents in a knowledge base – here Wikipedia and Wikidata – or not if the mention's referent is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eC4L37FtjYe-",
    "outputId": "26a08fc9-4511-4fdf-ee99-1fd75a7aca82"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!wget https://raw.githubusercontent.com/impresso/impresso-datalab-notebooks/refs/heads/main/entity/utils.py\n",
    "!wget https://raw.githubusercontent.com/impresso/impresso-datalab-notebooks/refs/heads/main/entity/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w5xfGodjxus0"
   },
   "outputs": [],
   "source": [
    "def print_nicely(results, text):\n",
    "    # Print the timestamp and system ID\n",
    "    print(f\"Timestamp: {results.get('ts')}\")\n",
    "    print(f\"System ID: {results.get('sys_id')}\")\n",
    "\n",
    "    entities = results.get('nes', [])\n",
    "    if entities:\n",
    "        print(f\"\\n{'Entity':<20} {'Type':<15} {'Confidence NER':<15} {'Confidence NEL':<15} {'Start':<5} {'End':<5} {'Wikidata ID':<10} {'Wikipedia Page':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "        for entity in entities:\n",
    "            confidence_ner = f\"{entity['confidence_ner']}%\"\n",
    "            confidence_nel = f\"{entity['confidence_nel']}%\"\n",
    "            wkd_id = entity.get('wkd_id', 'N/A')\n",
    "            wkpedia_pagename = entity.get('wkpedia_pagename', 'N/A')\n",
    "            print(f\"{entity['surface']:<20} {entity['type']:<15} {confidence_ner:<15} {confidence_nel:<15} {entity['lOffset']:<5} {entity['rOffset']:<5} {wkd_id:<10} {wkpedia_pagename:<20}\")\n",
    "\n",
    "        print(\"*\" * 100)\n",
    "        print('Testing offsets:')\n",
    "        print(\"*\" * 100)\n",
    "        print(f\"\\n{'Entity':<20} {'Type':<15} {'Confidence NER':<15} {'Confidence NEL':<15} {'Start':<5} {'End':<5} {'Wikidata ID':<10} {'Wikipedia Page':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "        for entity in entities:\n",
    "            confidence_ner = f\"{entity['confidence_ner']}%\"\n",
    "            confidence_nel = f\"{entity['confidence_nel']}%\"\n",
    "            wkd_id = entity.get('wkd_id', 'N/A')\n",
    "            wkpedia_pagename = entity.get('wkpedia_pagename', 'N/A')\n",
    "            print(f\"{text[entity['lOffset']:entity['rOffset']]:<20} {entity['type']:<15} {confidence_ner:<15} {confidence_nel:<15} {entity['lOffset']:<5} {entity['rOffset']:<5} {wkd_id:<10} {wkpedia_pagename:<20}\")\n",
    "\n",
    "        print(\"*\" * 100)\n",
    "        print('Testing offsets in the returned text:')\n",
    "        print(\"*\" * 100)\n",
    "        print(f\"\\n{'Entity':<20} {'Type':<15} {'Confidence NER':<15} {'Confidence NEL':<15} {'Start':<5} {'End':<5} {'Wikidata ID':<10} {'Wikipedia Page':<20}\")\n",
    "        print(\"-\" * 100)\n",
    "        for entity in entities:\n",
    "            confidence_ner = f\"{entity['confidence_ner']}%\"\n",
    "            confidence_nel = f\"{entity['confidence_nel']}%\"\n",
    "            wkd_id = entity.get('wkd_id', 'N/A')\n",
    "            wkpedia_pagename = entity.get('wkpedia_pagename', 'N/A')\n",
    "            print(f\"{results['text'][entity['lOffset']:entity['rOffset']]:<20} {entity['type']:<15} {confidence_ner:<15} {confidence_nel:<15} {entity['lOffset']:<5} {entity['rOffset']:<5} {wkd_id:<10} {wkpedia_pagename:<20}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGY-Vwvqxus0"
   },
   "source": [
    "Now the fun part, this function will download the requried model and gives you the keys to successfullly detect entities in your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "yXIIRNL3xus1",
    "outputId": "cc9e5e7b-61e9-4723-e93f-fca865698ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 507\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_nicely' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m get_linked_entities(sentence)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[0;32m---> 14\u001b[0m print_nicely(results, sentence)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_nicely' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import get_linked_entities\n",
    "\n",
    "sentence = \"\"\"In the year 1789, King Louis XVI, ruler of France, convened the Estates-General at the Palace of Versailles, \n",
    "                where Marie Antoinette, the Queen of France, alongside Maximilien Robespierre, a leading member of the National Assembly, \n",
    "                debated with Jean-Jacques Rousseau, the famous philosopher, and Charles de Talleyrand, the Bishop of Autun, \n",
    "                regarding the future of the French monarchy. At the same time, across the Atlantic in Philadelphia, \n",
    "                George Washington, the first President of the United States, and Thomas Jefferson, the nation's Secretary of State, \n",
    "                were drafting policies for the newly established American government following the signing of the Constitution.\"\"\"\n",
    "\n",
    "# Function to print each entry nicely\n",
    "def print_nicely(data):\n",
    "    for idx, entry in enumerate(data, start=1):\n",
    "        for key, value in entry.items():\n",
    "            print(f\"  {key.capitalize()}: {value}\")\n",
    "        print()  # Blank line between entries\n",
    "\n",
    "\n",
    "results = get_linked_entities(sentence)\n",
    "\n",
    "print(results)\n",
    "print_nicely(results, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYWMO8QFxus1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q9XStpV6ZI8"
   },
   "source": [
    "\n",
    "## About Impresso\n",
    "\n",
    "### Impresso project\n",
    "\n",
    "[Impresso - Media Monitoring of the Past](https://impresso-project.ch) is an\n",
    "interdisciplinary research project that aims to develop and consolidate tools for\n",
    "processing and exploring large collections of media archives across modalities, time,\n",
    "languages and national borders. The first project (2017-2021) was funded by the Swiss\n",
    "National Science Foundation under grant\n",
    "No. [CRSII5_173719](http://p3.snf.ch/project-173719) and the second project (2023-2027)\n",
    "by the SNSF under grant No. [CRSII5_213585](https://data.snf.ch/grants/grant/213585)\n",
    "and the Luxembourg National Research Fund under grant No. 17498891.\n",
    "\n",
    "### Copyright\n",
    "\n",
    "Copyright (C) 2024 The Impresso team.\n",
    "\n",
    "### License\n",
    "\n",
    "This program is provided as open source under\n",
    "the [GNU Affero General Public License](https://github.com/impresso/impresso-pyindexation/blob/master/LICENSE)\n",
    "v3 or later.\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/impresso/impresso.github.io/blob/master/assets/images/3x1--Yellow-Impresso-Black-on-White--transparent.png?raw=true\" width=\"350\" alt=\"Impresso Project Logo\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4_8lF2rxus2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeInYGAKxus2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIbSdkYbxus3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "115cX4hMxus3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLd95YnMxus3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLckw0Egxus3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABlst2l0xus3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
