{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Entities in a Text to Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entities such as organizations, locations, persons, and temporal expressions play a crucial role in the comprehension and analysis of both historical and contemporary texts. The HIPE-2022 project focuses on named entity recognition and classification (NERC) and entity linking (EL) in multilingual historical documents.\n",
    "\n",
    "### About HIPE-2022\n",
    "HIPE-2022 involves processing diverse datasets from historical newspapers and classical commentaries, spanning approximately 200 years and multiple languages. The primary goal is to confront systems with challenges related to multilinguality, domain-specific entities, and varying annotation tag sets.\n",
    "\n",
    "### Datasets\n",
    "The HIPE-2022 datasets are based on six primary datasets. The NEL model was only trained on all datasets with the exception of **letemps**.\n",
    "- **ajmc**: Classical commentaries in German, French, and English.\n",
    "- **hipe2020**: Historical newspapers in German, French, and English.\n",
    "- **letemps**: Historical newspapers in French.\n",
    "- **topres19th**: Historical newspapers in English.\n",
    "- **newseye**: Historical newspapers in German, Finnish, French, and Swedish.\n",
    "- **sonar**: Historical newspapers in German.\n",
    "\n",
    "### Annotation Types and Levels\n",
    "HIPE-2022 employs an IOB tagging scheme (inside-outside-beginning format) for entity annotations. The annotation levels include:\n",
    "\n",
    "1. **TOKEN**: The annotated token.\n",
    "2. **NE-COARSE-LIT**: Coarse type of the entity (literal sense).\n",
    "3. **NE-COARSE-METO**: Coarse type of the entity (metonymic sense).\n",
    "4. **NE-FINE-LIT**: Fine-grained type of the entity (literal sense).\n",
    "5. **NE-FINE-METO**: Fine-grained type of the entity (metonymic sense).\n",
    "6. **NE-FINE-COMP**: Component type of the entity.\n",
    "7. **NE-NESTED**: Coarse type of the nested entity.\n",
    "\n",
    "### Getting Started\n",
    "This notebook will guide you through setting up a workflow to identify named entities within your text using the HIPE-2022 trained pipeline. By leveraging this pipeline, you can detect mentions of people, places, organizations, and temporal expressions, enhancing your analysis and understanding of historical and contemporary documents.\n",
    "\n",
    "### Focus on Wikidata and Wikipedia Links\n",
    "In this notebook, we place a special focus on generating Wikidata and Wikipedia links for the identified named entities. By doing so, you can enrich your text with valuable metadata, providing deeper insights and connections to structured knowledge bases.\n",
    "\n",
    "---\n",
    "\n",
    "This updated description provides a clear overview of the HIPE-2022 project's goals, datasets, and annotation types, focusing on the identification of generic named entities in multilingual historical documents.\n",
    "*Note: This notebook *might* require `HF_TOKEN` to be set in the environment variables. You can get your token by signing up on the [Hugging Face website](https://huggingface.co/join) and read more in the [official documentation](https://huggingface.co/docs/huggingface_hub/v0.20.2/en/quick-start#environment-variable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries (if not already installed) and \n",
    "download the necessary NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13902,
     "status": "ok",
     "timestamp": 1712593966254,
     "user": {
      "displayName": "c2dh automaton",
      "userId": "06595644132255672638"
     },
     "user_tz": -120
    },
    "id": "eC4L37FtjYe-",
    "outputId": "b4126877-f531-4a91-ef9c-93b2d6e6a2e5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having installed the necessary libraries (if not already installed) we download the necessary NLTK data to run our POS tagger: **averaged_perceptron_tagger**.\n",
    "The averaged_perceptron_tagger is a efficient and effective part-of-speech (POS) tagger that basically tag each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc. See [https://arxiv.org/abs/2104.02831](https://arxiv.org/abs/2104.02831) as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1712594294715,
     "user": {
      "displayName": "c2dh automaton",
      "userId": "06595644132255672638"
     },
     "user_tz": -120
    },
    "id": "_UXo-c8aks_p",
    "outputId": "d1448337-3cc3-4ccc-de6b-b3edd251cb5c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nicely(results):\n",
    "    for key, entities in results.items():\n",
    "        if entities:\n",
    "            print(f\"\\n**{key}**\\n\")\n",
    "            print(f\"{'Entity':<15} {'Type':<10} {'Score':<8} {'Index':<5} {'Word':<20} {'Start':<5} {'End':<5}\")\n",
    "            print(\"-\" * 70)\n",
    "            for entity in entities:\n",
    "                print(f\"{entity['word']:<15} {entity['entity']:<10} {entity['score']:<8.4f} {entity['index'][0]:<5} {entity['word']:<20} {entity['start']:<5} {entity['end']:<5}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we detect the entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Define the model name to be used for token classification, we use the Impresso NER\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual\"\n",
    "\n",
    "# Load the tokenizer corresponding to the specified model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for named entity recognition (NER) using the loaded model and tokenizer\n",
    "nlp = pipeline(\"generic-ner\", model=MODEL_NAME, tokenizer=tokenizer, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"\"\"Apple est créée le 1er avril 1976 dans le garage de la maison \n",
    "            d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak \n",
    "            et Ronald Wayne, puis constituée sous forme de société le 3 janvier 1977 à l'origine \n",
    "            sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification \n",
    "            de ses produits, le mot « computer » est retiré le 9 janvier 2015.\n",
    "            \"\"\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Input text to be processed for named entity recognition\n",
    "    print(f'Sentence: {sentence}')\n",
    "    results = nlp(sentence)\n",
    "    print_nicely(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we link them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entity_tags(text, results):\n",
    "    entities = []\n",
    "    for task, ents in results.items():\n",
    "        for entity in ents:\n",
    "            entities.append((entity['start'], entity['end'], entity['word']))\n",
    "\n",
    "    already_done = []\n",
    "    # Insert tags in the text using replace method\n",
    "    for start, end, word in entities:\n",
    "        if word not in already_done:\n",
    "            text = text.replace(word, f\"[START] {word} [END]\")\n",
    "            already_done.append(word)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules from the transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model from the specified pre-trained model name\n",
    "# The model used here is \"impresso-project/nel-mgenre-multilingual\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"impresso-project/nel-mgenre-multilingual\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"impresso-project/nel-mgenre-multilingual\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each sentence for named entity recognition and linking\n",
    "for sentence in sentences:\n",
    "    # Input text to be processed for named entity recognition\n",
    "    print(f'Sentence: {sentence}')\n",
    "    \n",
    "    # Run the NER pipeline on the input sentence and store the results\n",
    "    results = nlp(sentence)\n",
    "\n",
    "    # Initialize a list to hold the entities\n",
    "    entities = []\n",
    "\n",
    "    # Extract entities from the results\n",
    "    for task, ents in results.items():\n",
    "        for entity in ents:\n",
    "            entities.append((entity['start'], entity['end'], entity['word']))\n",
    "    \n",
    "    # List to keep track of already processed words to avoid duplicate tagging\n",
    "    already_done = []\n",
    "\n",
    "    # Process each entity for linking\n",
    "    for start, end, word in entities:\n",
    "        if word not in already_done:\n",
    "            # Tag the entity in the text\n",
    "            entity_text = sentence.replace(word, f\"[START] {word} [END]\")\n",
    "            print(f\"\\nEntity: {word}, Tagged Text: {entity_text}\\n\")\n",
    "\n",
    "            # Generate Wikipedia links for the tagged text\n",
    "            outputs = model.generate(\n",
    "                **tokenizer([entity_text], return_tensors=\"pt\"),\n",
    "                num_beams=5,\n",
    "                num_return_sequences=5\n",
    "            )\n",
    "            \n",
    "            # Decode the generated output to get the Wikipedia links\n",
    "            wikipedia_links = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            print(f\"\\nEntity: {word}, Wikipedia Links: {wikipedia_links}\")\n",
    "            \n",
    "            # Add the word to the already processed list\n",
    "            already_done.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_wikipedia_page_props\n",
    "\n",
    "# Process each sentence for named entity recognition and linking\n",
    "for sentence in sentences:\n",
    "    # Input text to be processed for named entity recognition\n",
    "    print(f'Sentence: {sentence}')\n",
    "    \n",
    "    # Run the NER pipeline on the input sentence and store the results\n",
    "    results = nlp(sentence)\n",
    "\n",
    "    # Initialize a list to hold the entities\n",
    "    entities = []\n",
    "\n",
    "    # Extract entities from the results\n",
    "    for task, ents in results.items():\n",
    "        for entity in ents:\n",
    "            entities.append((entity['start'], entity['end'], entity['word']))\n",
    "    \n",
    "    # List to keep track of already processed words to avoid duplicate tagging\n",
    "    already_done = []\n",
    "\n",
    "    # Process each entity for linking\n",
    "    for start, end, word in entities:\n",
    "        if word not in already_done:\n",
    "            # Tag the entity in the text\n",
    "            entity_text = sentence.replace(word, f\"[START] {word} [END]\")\n",
    "            print(f\"\\nEntity: {word}, Tagged Text: {entity_text}\\n\")\n",
    "\n",
    "            # Generate Wikipedia links for the tagged text\n",
    "            outputs = model.generate(\n",
    "                **tokenizer([entity_text], return_tensors=\"pt\"),\n",
    "                num_beams=5,\n",
    "                num_return_sequences=5\n",
    "            )\n",
    "            \n",
    "            # Decode the generated output to get the Wikipedia links\n",
    "            wikipedia_links = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            print(f\"\\nEntity: {word}, Wikipedia Links: {wikipedia_links}\")\n",
    "            \n",
    "            # Add the word to the already processed list\n",
    "            already_done.append(word)\n",
    "            \n",
    "            # Retrieve and print Wikidata QID for each Wikipedia link\n",
    "            for wikipedia_link in wikipedia_links:\n",
    "                qid = get_wikipedia_page_props(wikipedia_link)\n",
    "                print(f\"  Wikidata: {wikipedia_link} -> {qid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKXk4sqe0yBXtOpC+Z4ngA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
